<experiment>
<parameters>
	<episodes value="1000" />
	<game-duration value="3000" /> 
	<abstraction-model value="aggregatediff" />
	<output-dir value="/tmp/results" />
	<reward-function value="winloss" />
	<quiet-learning value="true" />
	<debug-level value="0" />
</parameters>

<player name='MetaBot' type='MetaBotAIR1'>
	<path-to-knowledge value="/home/lancs/anbalaga/micrortsFA-master/files/q_MetaBot_final2.txt"></path-to-knowledge>
	<learning-rate-meta type="exponential-decay" initial="0.001" final="0.001" rate="1" /> <!-- From 1.0 to 0.01 in 55000 episodes -->
	<epsilon value="0.3"></epsilon>
	<decay-rate value="0.9996001"></decay-rate></player>

<player name='MetaBot2' type='MetaBotAIR1'>
        <path-to-knowledge value="/home/lancs/anbalaga/micrortsFA-master/files/q_MetaBot_final.txt"></path-to-knowledge>
        <learning-rate-meta type="exponential-decay" initial="0" final="0" rate="1" /> <!-- From 1.0 to 0.01 in 55000 episodes -->
        <epsilon value="0"></epsilon>
        <decay-rate value="1"></decay-rate></player>

</experiment>
