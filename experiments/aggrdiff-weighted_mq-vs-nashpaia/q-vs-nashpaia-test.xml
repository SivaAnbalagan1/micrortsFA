
<experiment>
	<parameters>
		<episodes value="100" />
		<game-duration value="3000" /> 
		<abstraction-model value="aggregatediff" />
		<output-dir value="/tmp/results" />
		<reward-function value="simpleweighted" />
		<quiet-learning value="true" />
		<debug-level value="0" />
	</parameters>

	<player name="QLearning" type="SGQLearningAdapter">
		<discount value='0' />
		<learning-rate type='constant' value='0' />
		<initial-q value='0' />
		<epsilon value='0' />
		<!-- <path-to-knowledge value="specify/via/cmd/line" /> -->
	</player>

	<player name='Nash-PAIA' type='NashPortfolioAIAdapter'>
		<timeout value="5000" />
		<playouts value="36" />
		<lookahead value="200" />
		<evaluation-function value="SimpleSqrtEvaluationFunction3" />
	</player>

</experiment>