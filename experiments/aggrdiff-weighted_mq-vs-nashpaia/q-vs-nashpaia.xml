
<experiment>
	<parameters>
		<episodes value="5000" />
		<game-duration value="3000" /> 
		<abstraction-model value="aggregatediff" />
		<output-dir value="/tmp/results" />
		<reward-function value="simpleweighted" />
		<quiet-learning value="true" />
		<debug-level value="0" />
	</parameters>

	<player name="QLearning" type="SGQLearningAdapter">
		<discount value="0.9" />
		<learning-rate type="exponential-decay" initial="1.0" final="0.01" rate="0.999916273" /> <!-- From 1.0 to 0.01 in 55000 episodes -->
		<initial-q value="1" />
	</player>

	<player name='Nash-PAIA' type='NashPortfolioAIAdapter'>
		<timeout value="5000" />
		<playouts value="36" />
		<lookahead value="200" />
		<evaluation-function value="SimpleSqrtEvaluationFunction3" />
	</player>

</experiment>