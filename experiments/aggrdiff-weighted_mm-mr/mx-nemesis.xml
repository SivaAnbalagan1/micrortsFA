<!-- 
	This can be used to train player 2 against MM or MR resulting policy.
	We just need to specify which policy to be loaded by player 1 via
	command line
-->

<experiment>
	<parameters>
		<episodes value="60000" />
		<game-duration value="3000" /> 
		<abstraction-model value="aggregatediff" />
		<output-dir value="/tmp/results" />
		<reward-function value="simpleweighted" />
		<quiet-learning value="true" />
	</parameters>

	<player name="QLearning1" type="MinimaxQ">
		<discount value='0' />
		<learning-rate type='constant' value='0' />
		<initial-q value='0' />
		<epsilon value='0' />
		<!-- <path-to-knowledge value="specify/via/cmd/line" /> -->
	</player>

	<player name="MNemesis" type="SGQLearningAdapter">
		<discount value="0.9" />
		<learning-rate type="exponential-decay" initial="1.0" final="0.01" rate="0.999916273" /> <!-- From 1.0 to 0.01 in 55000 episodes -->
		<initial-q value="1" />
	</player>

</experiment>
